input{
  kafka{
    #这里设置为kafka集群的ip
    bootstrap_servers => "10.0.19.26:9092,10.0.19.26:9093,10.0.19.26:9094"
    #可以使用topics明确地表示订阅的主题是什么，这个主题是在filebeat中设置的
    #topics => ["test"]
    #也可以通过设立topics_pattern对同属于这种类型的文件进行处理，例如下面的就对都是java类型处理
    topics_pattern => "java[a-zA-Z0-9\-]*"
    #按照订阅数大概设置线程数量
    consumer_threads => 5
    #设置group_id用于logstash集群，要注意集群间group_id应一致
    group_id => "logstash"
    #因为filebeat端过来的是json形式，所以这样接收
    codec=>"json"
  }
  
}

filter{
  #抓取的插件,这里应注意若是要match多样东西，要开多一个grok，不然不会拼在一起的
  grok{
    #设置正则表达式的模式文件夹，记住这个应在docker-compose文件中挂载上去
    patterns_dir => ["/etc/logstash/patterns"]
    #按照写好的正则表达式进行匹配，这里也可以将表达式变为%{XXX:xxx}的形式，将XXX放入正则表达式模式里面即可
    match => [
      "message","\s*%{TIMESTAMP_ISO8601:time}\s*%{LOGLEVEL:loglevel}\s*[0-9]*\s*---\s*\[\s*%{RUNNING_THREAD:thread}\]\s*%{JAVACLASS:class}\s*:\s*%{MSG:msg}"
      ]
  }
  grok{
    #设置正则表达式的模式文件夹，记住这个应在docker-compose文件中挂载上去
    patterns_dir => ["/etc/logstash/patterns"]
    #按照写好的正则表达式进行匹配，这里也可以将表达式变为%{XXX:xxx}的形式，将XXX放入正则表达式模式里面即可
    match => [
      #对source即filebeat发过来的log文件地址进行处理，提取到地区、服务名以及文件名。
      "source","/path/to/log/%{WORDS:area}/%{WORDS:service}/log/%{WORDS:fileName}"
      ]
  }

  #替换、处理
  mutate {
    # 处理掉返回的消息多出的字段【认为不重要的字段，以顶级字段来移除】
    remove_field=>["beat","input","message"]
    # 对获取到的meassage进行处理，因为在处理时，会将换行处理为文本形式的\\n，所以这里就将其转为换行【就回车符号】
    gsub => ["message","\\n","
"]}
}

output{
  #匹配不成功的例子就不加进来elasticsearch中了，消耗资源
  if "_grokparsefailure" not in [tags] {
    elasticsearch {
   	 #发布给elasticsearch的server
   	 hosts => ['10.0.19.26:9200','10.0.19.26:9201']
   	 #设置index，用于告诉kibana这个output的东西叫什么名字
   	 index => "%{+YYYY.MM.dd}-log"
    }
  }
}
