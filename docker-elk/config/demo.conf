input{
  kafka{
    bootstrap_servers => "10.0.19.26:9092,10.0.19.26:9093,10.0.19.26:9094"
    #看你在filebeat中设置的topic是什么
    #bootstrap_servers => "172.16.11.119:9092"
    #topics => ["im","permission","thirdapi","casecenter","archivecenter","interrogate","faceapi","cloudconfig","eureka"]
    topics => ["huadu","huangpu","linhe","nanshan","zengcheng"]
    consumer_threads => 7
    group_id => "logstash"
    codec=>"json"
  }
  
}

filter{
  grok{
    patterns_dir => ["/etc/logstash/patterns"]
    match => {
      "message" => "\s*%{TIMESTAMP_ISO8601:time}\s*%{LOGLEVEL:loglevel}\s*[0-9]*\s*---\s*\[\s*%{RUNNING_THREAD:thread}\]\s*%{JAVACLASS:class}\s*:\s*%{MSG:msg}"
    }
  }
  mutate {
    remove_field=>["beat","message","input"]
    gsub => ["message","\\n","
"]}
}

output{
  elasticsearch {
    #发布给elasticsearch的server
    hosts => ['172.16.11.119:9200','172.16.11.119:9201']
    #hosts => ['elasticsearch']
    #设置index，用于告诉kibana这个output的东西叫什么名字
    index => "%{+YYYY.MM.dd}-log"
    #index => "%{topics}-%{+YYYY.MM.dd}-log"
  }
}
