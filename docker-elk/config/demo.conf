input{
  kafka{
    #这里设置为kafka集群的ip
    bootstrap_servers => "172.16.16.10:9092,172.16.16.10:9093,172.16.16.10:9094"
    #可以使用topics明确地表示订阅的主题是什么，这个主题是在filebeat中设置的
    topics => ["test"]
    #按照订阅数大概设置线程数量
    consumer_threads => 5
    #设置group_id用于logstash集群，要注意集群间group_id应一致
    group_id => "logstash"
    #因为filebeat端过来的是json形式，所以这样接收
    codec=>"json"
  }
  
}

filter{
  #抓取的插件
  grok{
    #设置正则表达式的模式文件夹，记住这个应在docker-compose文件中挂载上去
    patterns_dir => ["/etc/logstash/patterns"]
    #按照写好的正则表达式进行匹配，这里也可以将表达式变为%{XXX:xxx}的形式，将XXX放入正则表达式模式里面即可
    match => {
      "message" => "\s*%{TIMESTAMP_ISO8601:time}\s*%{LOGLEVEL:loglevel}\s*[0-9]*\s*---\s*\[\s*%{RUNNING_THREAD:thread}\]\s*%{JAVACLASS:class}\s*:\s*%{MSG:msg}"
    }
  }
  #替换、处理
  mutate {
    # 处理掉返回的消息多出的字段【认为不重要的字段，以顶级字段来移除】
    remove_field=>["beat","message","input"]
    # 对获取到的meassage进行处理，因为在处理时，会将换行处理为文本形式的\\n，所以这里就将其转为换行【就回车符号】
    gsub => ["message","\\n","
"]}
}

output{
  elasticsearch {
    #发布给elasticsearch的server
    hosts => ['172.16.16.10:9200','172.16.16.10:9201']
    #设置index，用于告诉kibana这个output的东西叫什么名字
    index => "%{+YYYY.MM.dd}-log"
  }
}
