input{
  kafka{
    #zk_connect is version 2's configure
    #bootstrap_servers => "localhost:9092,localhost:9093,localhost:9094"
    #看你在filebeat中设置的topic是什么
    bootstrap_servers => "localhost:9092"
    topics => ["cmt_server"]
    enable_auto_commit => true
    auto_commit_interval_ms => "1000"
    consumer_threads => 4
    #client_id => "logstash-client1"
    group_id => "logstash-group1"
  }
}

filter{
  #grok{
  #定义正则使用的pattern文件夹，预定义，也可以直接在match中写正则
  #  patterns_dir => ["/etc/logstash/patterns"]
  #  match => {"message" => "%{DATESTAMP:timestamp}  %{LOGLEVEL:loglevel} 24 --- \[%{RUNNING_THREAD:thread}\] %{JAVA_PACKAGE:package} *: %{CUSTOM_MSG:msg}"}
  #}
  #date{
  #  timezone => "Asia/Shanghai"
  #}
}


output{
  elasticsearch {
    #发布给elasticsearch的server
    hosts => ["127.0.0.1:9200"]
    #设置index，用于告诉kibana这个output的东西叫什么名字
    index => "log-%{+YYYY.MM.dd}"
    #index => "demo3"
  }
}
